\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[brazil]{babel}
\usepackage{graphicx}
\usepackage{makeidx}
\usepackage{spverbatim}
\makeindex


%opening
\date{\today}
\begin{document}
    \begin{titlepage}
        \begin{figure}
            \centering
          \includegraphics[width=7cm,keepaspectratio=true]{imagens/unicamp.png}

        \end{figure}
        \begin{center}
            \huge{Universidade Estadual de Campinas}

        \vfill
        \textbf{\LARGE{Manual}}
        \vfill
        \end{center}

        \begin{flushleft}
            \begin{tabbing}
                Autores\qquad\qquad\= Gabriel Bueno de Oliveira 139455 \\
                    \>Joao Guilherme Daros Fidelis 136242 \\
                    \>Lucas Henrique Morais 136640 \\
                    \>Matheus Yokoyama Figueiredo 137036\\
                    \>Pedro Rodrigues Grijo 139715\\
            \end{tabbing}
        \end{flushleft}
    \end{titlepage}

\newpage
\section{Setup do Cluster}
    \subsection{Acesso}
    O acesso ao cluster pode ser feito de fora e de dentro do IC (Instituto de Computação - UNICAMP).
        \subsubsection{Interno ao IC}
        Para acceso interno ao ic, basta acessar por ssh o endereço cbn6.lab.ic.unicamp.br. No linux, isso pode ser feito através de:
        \begin{spverbatim}
        # ssh cbn6.lab.ic.unicamp.br
        \end{spverbatim}

        Após a conexão ser feita, insira sua senha.
        \subsubsection{Externo ao IC}
        Para o acesso externo, é necessário fazer uma conexão de duas fases ou usar um tunnel por uma máquina do ic. Para acesso remoto a uma máquina do ic, no linux, utilize \textbf{um} dentre os seguintes comandos:
        \begin{spverbatim}
        # ssh -l raXXXXXX ssh.students.ic.unicamp.br
        # ssh -l raXXXXXX ssh2.students.ic.unicamp.br
        \end{spverbatim}

        Após a conexão ser feita, insira sua senha. Após feita a credencial, prossiga do mesmo modo como se tivesse acessando de dentro do IC.

\section{Instalação e Configuração}
    \subsection{PostgreSQL}
        \subsubsection{Instalação}
        A versão utilizada do PostgreSQL é a versão 9.5, que pode ser obtida seguindo os seguintes comandos:
        \begin{spverbatim}
        # sudo su
        # cat /etc/apt/sources.list.d/postgresql.list
        # deb http://apt.postgresql.org/pub/repos/apt/ trusty-pgdg main
        # sudo apt-get install wget ca-certificates
        # wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
        # sudo apt-get update
        # sudo apt-get upgrade
        # sudo apt-get install postgresql-9.5

        \end{spverbatim}

        \subsubsection{Configuração}
        Para configurar o acesso ao banco de dados deve-se alterar o arquivo de configuração principal. Nele são definidos toda a configuração de acesso ao banco de dados e os diretórios de dados que são respectivamente:

        \begin{spverbatim}
        hba_file = '/etc/postgresql/9.5/main/pg_hba.conf'
        data_directory = '/var/lib/postgresql/9.5/main'
        \end{spverbatim}

        O arquivo pg\_hba.conf define regras para a conexão ao banco. Por padrão, são só permitidas conexões locais: o usuário administrador do banco (postgres, quando instalado por gerenciador de pacotes) tem acesso via socket e sem senha. os demais usuários, via interface local e com senha:
        \begin{spverbatim}
        local   all         postgres            peer

        host    all         all         127.0.0.1/32            md5
        \end{spverbatim}

    Para criar o primeiro usuário siga os seguintes comandos:
        \begin{spverbatim}
            # sudo su
            # su postgres
            # psql
            =# CREATE ROLE <usr_name> with superuser createdb createrole login;
            =# ALTER ROLE <usr_name> PASSWORD ‘<password>’;
        \end{spverbatim}

        \subsubsection{Carregando o Dump}
        Para carregar o ddump no banco deve-se criar o banco de dados (tpcw) e um usuário tpcw-user.
        \begin{spverbatim}
        # pg_restore -d tpcw tpcw-database-dump.tar
        \end{spverbatim}


    \subsection{Tomcat}
        \subsubsection{Instalação}
        Para instalar o Tomcat (usaremos o tomcat7), precisamos instalar o pacote do tomcat e suas dependencias. Para isso, no linux, podemos usar o apt-get:
        \begin{spverbatim}
        # sudo apt-get install tomcat7
        \end{spverbatim}

        A instalação irá criar um usuário tomcat7 dentro de um grupo, tomcat7. Os arquivos estão em \$CATALINA\_BASE, que na instalação do Ubuntu é /var/lib/tomcat7.

        Para teste, acesse em uma máquina do IC o cbn6.lab.ic.unicamp.br:8080.

        \subsubsection{Integração com o PostgreSQL}
        Para integração com o PostgreSQL é necessário fazer o download do .jar do PostgreSQL JDBC para ser adicionado a pasta \$CATALINA\_BASE/lib. Após fazer o download, modifique os seguintes arquivos com o conteúdo indicado:

        \begin{itemize}
        \setlength{\itemindent}{-.0in}
            \item \$CATALINA\_BASE/conf/context.xml: http://www.ggte.unicamp.br/eam/pluginfile.php/188108/
            mod\_wiki/attachments/132/tomcat7-context.xml
            \item \$CATALINA\_BASE/conf/server.xml: http://www.ggte.unicamp.br/eam/pluginfile.php/188108/
            mod\_wiki/attachments/132/tomcat7-server.xml. Para este, modifique os campos de username, password e possivelmente url.
        \end{itemize}

        Após essas configurações, reinicie o tomcat:

        \begin{spverbatim}
        # invoke-rc.d tomcat7 restart
        \end{spverbatim}

        \subsubsection{Arco 03} 
        
        Devido a problemas relacionados à pool de conexões quando há uma queda de um dos bancos, houve a necessidade de se checar a validade de cada conexão do pool. Para isto, novas configurações foram usadas para o Tomcat JDBC, como podem ser vista na Figura 1 acima. Nota-se que as flags de log foram usadas apenas por motivos de depuração.
       
        \begin{center}
        \begin{figure}
            \centering
          \includegraphics[width=14cm,keepaspectratio=true]{imagens/config_tomcat.png}
	        	        Figura 1: Configurações do Tomcat no Arco 03
        \end{figure}
	\end{center}
        
        \subsection{TPC-W}

        As aplicações implantadas no Tomcat são configuradas em \$CATALINA\_BASE/webapps. Para ter permissão de criar aplicações nesta pasta, o usuário tem que ser do grupo tomcat7, para fazer isso, no linux:

        \begin{spverbatim}
        # adduser [usuário_comum] tomcat7
        \end{spverbatim}

        Para instalação default do TPC-W no tomcat7, crie a seguinte árvore:
        \begin{spverbatim}

            \$CATALINA_BASE/webapps

                tpcw

                     Images

                     WEB-INF

                        classes

                        web.xml


        \end{spverbatim}

        Na pasta tpcw, rode os seguintes comandos:
        \begin{spverbatim}
            # sudo cp /nfs/servlet/Images.tar.gz .
            # sudo tar -xzf Images.tar.gz
            # sudo cp /nfs/servlet/tpcw-servlets.tar.gz
            # sudo tar -xzf tpcw-servlets.tar.gz
            # cd tpcw-servlets.tar.gz
            # sudo make
        \end{spverbatim}

        No fim, copie todas as *.class geradas para a pasta WEB-INF/classes e copie o web.xml para pasta WEB-INF/. Reinicie o Tomcat.

       \subsection{RBE}
        O RBE (Remote Browser Emulator) é o programa empregado para gerar carga para o TPC-W. É um aplicativo que emula um conjunto de clientes que acessam o lado servidor do TPC-W, que implementa uma loja de livros. Os clientes operam como se fossem navegadores web.
        O código fonte do RBE, assim como arquivo README e o script em Python que realiza a análise das saídas estão disponíves na pasta do RBE.

        \subsubsection{Automatização}
        Para facilitar a coleta dos dados foi implementado um script em bash que executa o rbe com variando os parâmetros e as saídas, já executa o script analyse.py e gera os gráficos.
        Para executar o script basta executar:
        \begin{spverbatim}
        # ./script.sh
        \end{spverbatim}

\section{Entrando e conhecendo as novas máquinas}
    Obtivemos acesso a duas máquinas diferentes do cluster que contém uma instância do postgres cada, assim, uma máquina conterá o banco primário e a outra o secundário.\\
    O banco primário fica na máquina dbmaster2. Essa máquina tem IP 10.1.2.10 e o banco fica na porta 5434.\\
    Já o banco secundário fica na máquina dbslave2. Essa máquina tem IP 10.1.2.20 e o banco fica na porta 5434.

    Para acessar o dbmaster2, basta enquanto logado no cluster cbn6, dar ssh no seu endereço, assim:
    \begin{spverbatim}
    # ssh dbmaster2
    \end{spverbatim}

    Para acessar o dbslave2, basta enquanto logado no cluster cbn6, dar ssh no seu endereço, assim:
    \begin{spverbatim}
    # ssh dbslave2
    \end{spverbatim}

    As máquinas já vieram com a instalação do PostgreSQL 9.5.1.

    As pastas de dados para os bancos de dados em ambas as máquinas fica em \verb|/var/lib/postgresql/9.5/grupo06/|.

    As pastas de configurações (como postgres.conf e pg\_hba.conf), ficam em \verb|/etc/postgresql/9.5/grupo06/|.

    Também é necessário configurar e iniciar um banco de dados inicial no banco primário em dbmaster2.

    Faça login na máquina dbmaster2 e crie um usuário "tpcw-user" como é ensinado na seção 2.1.2 e faça uma restauração do dump como na seção 2.1.3.

\section{Instalando a Replicação}
    Era necessário instalar o serviço de Streaming Replication do banco primário para o secundário, para que mudanças feitas no banco primário sejam copiadas para o secundário. Para isto, seguimos tutoriais sugeridos pelo professor.
    Agora, temos dois bancos de dados em duas máquinas diferentes.
    \subsection{Editar configurações no mestre}
      Na máquina do banco primário, dbmaster2, vamos criar um usuário "replication" no postgres, que é um usuário com privilégios de replicação. O usuário terá a senha 123456. Para isso, execute os comandos:
      \begin{spverbatim}
      # sudo su
      # su postgres
      # psql
      # CREATE ROLE replication WITH REPLICATION PASSWORD ‘123456’ LOGIN
      \end{spverbatim}

      Agora, vamos editar o arquivo /etc/postgresql/9.5/grupo06/postgresql.conf da máquina dbmaster2.
      Na seção \textbf{CONNECTIONS AND AUTHENTICATION}, adicione a linha:
      \begin{spverbatim}
      listen_addresses = '*'
      \end{spverbatim}

      Na seção \textbf{WRITE AHEAD LOG}, adicione as linhas:
      \begin{spverbatim}
      wal_level = hot_standby
      \end{spverbatim}

      Em \textbf{REPLICATION}, coloque as linhas:
      \begin{spverbatim}
      max_wal_senders = 5
      wal_keep_segments = 32
      \end{spverbatim}

      Finalmente, em \textbf{ARCHIVING}, adicionamos as linhas:
      \begin{spverbatim}
      archive_mode = on
      archive_command = 'test ! -f /var/lib/postgresql/9.5/grupo06/archivedir/\%f && cp \%p /var/lib/postgresql/9.5/grupo06/archivedir/\%f'
      \end{spverbatim}

     Depois, crie a pasta do comando acima com:
      \begin{spverbatim}
      # mkdir /var/lib/postgresql/9.5/grupo06/archivedir/
      \end{spverbatim}

      Veja que a pasta /var/lib/postgresql/9.5/grupo06/archivedir/ será criada e nela ficará contido todos os arquivos log gerados pelos bancos de dados.
      Agora, temos que editar o arquivo /etc/postgresql/9.5/grupo06/pg\_hba.conf. Adicione a linha no final:
      \begin{spverbatim}
      host    replication     replication     10.1.2.20/32    md5
      \end{spverbatim}

      Essa linha permite que o banco em standby consiga se conectar com o primário para fazer a replicação.
    \subsection{Fazer base backup}
        Devemos fazer um base backup do servidor mestre para o escravo. Utilizaremos o comando pg\_basebackup.\\
        Para isso, faça login na máquina do escravo, a dbslave2.\\
        Primeiro, a pasta de dados do banco de dados que receberá o base backup deve estar vazia. Logo, apagaremos tudo dentro dela. Depois, utilizaremos o comando de base backup para se conectar no banco de dados mestre e fazer a cópia.\\
        Rode o comando:
        \begin{spverbatim}
        rm /var/lib/postgresql/9.5/grupo06/* -r
        pg_basebackup -h 10.1.2.10 -p 5434 -D /var/lib/postgresql/9.5/grupo06 -P -U replication --xlog-method=stream -R
        \end{spverbatim}

        Ao ser pedido uma senha, digite 123456.\\
        A flag -R no final, faz com que o arquivo recovery.conf seja gerado automaticamente no processo.

    \subsection{Configurar o slave}
        Na máquina escrava, edite o arquivo \verb|/etc/postgresql/9.5/grupo06/postgresql.conf|, adicionando as mesmas linhas que fez na máquina mestre, como nos passos anteriores. Isso é necessário pois quando o primário falhar, o secundário deve se comportar exatamente como o primário. Entretanto, é necéssario adicionar mais uma linha, na seção \textbf{STANDBY SERVERS}, adicione a linha:
        \begin{spverbatim}
        hot_standby = on
        \end{spverbatim}

        Isso faz com que o banco escravo seja read-only e aceite apenas comandos de leitura.
        
    \subsection{Arco 03}
    
        Para as configurações de replicação do postgres, foi adicionado ao arquivo recovery.conf a seguinte linha:
        \begin{spverbatim}
        recovery_target_timeline = 'latest'
        \end{spverbatim}
    
        Assim, após a queda, promoção e possíveis correções de consistência pelo \verb|pg_rewind|, o banco que agora seria o secundário seguiria pegando na mais nova timeline criada pelo banco promovido, agora master.
    
    
    Para habilitar o \verb|pg_rewind|, precisamos fazer modificações no arquivo
    
    \verb|/etc/postgresl/9.5/grupo06/postgresql.conf| das duas máquinas, dbmaster2 e dbslave2. Adicionamos na seção WRITE AHEAD LOG a linha:
        
        \begin{spverbatim}
        wal_log_hints = on
        \end{spverbatim}

\section{Configuração do HAProxy}

    \subsection{Instalação}

        Para instalar o HAProxy, tivemos apenas que adicionar ao nosso ambiente de execução o repositório apt-get relacionado e rodar o comando apt-get install apropriado, como segue:

        \begin{spverbatim}
        # sudo add-apt-repository ppa:vbernat/haproxy-1.6
        # sudo apt-get update
        # sudo apt-get install haproxy
        \end{spverbatim}

        Podemos verificar que a versão correta foi então instalada com o seguinte comando:

       \begin{spverbatim}
        # haproxy -v
        \end{spverbatim}

    \subsection{Configuração}

        Uma vez instalado o HAPROXY, temos ainda de configurá-lo de modo a:

        \begin{enumerate}
        \item Informá-lo da existência de dois endereços de destino distintos
        \item Permitir que seu backend escolha o endereço de destino adequado de acordo com o resultado de um health-check periódico de cada um dos alvos
        \item //Requisitar que o HAProxy escreva mensagens de log relevantes em /var/log
        \end{enumerate}

        Para tal, é necessário criar em \verb|/etc/haproxy| um arquivo haproxy.cfg com parâmetros adequados.
        A seguir, comentamos o conteúdo de um arquivo mínimo desse tipo que é capaz de prover o requerido em (1) e (2):

       \begin{spverbatim}
        listen psql

       bind    *:4002

       mode  tcp

       option  pgsql-check user haproxy

       server  replicaOne  10.1.2.10:5434 check

       server  replicaTwo  10.1.2.20:5434 check backup
        \end{spverbatim}

        A linha 02 garante que toda a comunicação ocorrendo através da porta 4002 deverá ser interceptada.

        A linha 03 indica  que o haproxy deverá funcionar como um tcp proxy – isto é, transparentemente interceptando e redirecionando pacotes do tipo TCP.

        A linha 04 configura o haproxy para fazer contatos de health-check com os bancos replicados usando o usuário haproxy.

        As linhas 05 e 06 definem as réplicas do banco como os dois destinos possíveis para os pacotes TCP interceptados. Além disso, a opção “backup” da linha 06 garante que o servidor replicaTwo não seja contatado antes que replicaOne falhe.

        Por fim, as opções “check” ocorrendo nas duas linhas indicam que haproxy deverá verificar periodicamente a disponibilidade dos dois bancos, alternando o destino dos pacotes interceptados de acordo com essa avaliação.

    \subsection{Startup, restart e shutdown}

        Para realizar o startup, restart e shutdown do HAProxy, respectivamente, basta usar o comando “service” do Linux, como segue:

        \begin{spverbatim}
        sudo service haproxy start
        sudo service haproxy restart
        sudo service haproxy stop
        \end{spverbatim}

    \subsection{Arco 03}

    Como a detecção de falhas foi passada para um novo script(descrito na seção de Detector de falhas abaixo), o check do haproxy se tornou obsoleto, servindo apenas para checagem na GUI no site disponibilizado pelo haproxy.

Então as configurações de haproxy foram separadas em dois arquivos, onde um deles aponta para o dbmaster2 e outro para dbslave2. Quem ficou resposável pela troca de configuração e religar o haproxy também ficou por parte do detector de falhas.


\section{Detector de falhas}

	Para a implementação do detector de falhas bolamos um script em bash que executa uma consulta em loop dos status dos banco de dados. Ao detectar que algum dos bancos de dados caiu ele automaticamente, se for o primário, altera as configurações do haproxy e realiza a promoção do banco de dados secundário para primário e restaura o banco que estava indisponível como secundário. Para a execução do script deve-se estar logado com um usuário pertencente ao grupo e executar o seguinte comando:

       \begin{spverbatim}
# ./health_checker.sh
        \end{spverbatim}

\section{Automatização da coleta de dados}

            Todos os scripts mencionados nessa seção estão na pasta \verb|scripts|.

	A segunda parte do experimento foi dividida em 3 partes. Para executar os testes da primeira parte, foram gerados os scripts \verb|arc2_exp1_script.sh| e \verb|arc2_exp_analise.sh| que devem ser executados da seguinte forma:

 	\begin{spverbatim}
	# ./arc2_exp1_script.sh
	# ./arc2_exp_analise.sh 1
	\end{spverbatim}
\noindent
\newline
O argumento passado para o segundo script é simplesmente para geração de arquivos com o nome desejado.
\newline

Para executar os testes da segunda parte, foi gerado o script \verb|arc2_exp2_script.sh|. Seguem os comandos para executar:
	\begin{spverbatim}
	# ./arc2_exp2_script.sh
	# ./arc2_exp_analise.sh 2
	\end{spverbatim}
\noindent
\newline
Para executar os testes da terceira parte, foi gerado o script \verb|arc2_exp3_script.sh|. Seguem os comandos para executar
	\begin{spverbatim}
	# ./arc2_exp3_script.sh
	# mkdir ../rbe/arc2/exp3/analise
	# python ../rbe/ananlyse.py <outs_rbe>
	\end{spverbatim}

    \subsection{Arco 03}

        O programa necessário para executar o experimento se encontra em /ra137036/grupo06/programas e para a execução verifique que não há nenhum \verb|health_checker.sh| rodando. Para executar o experimento do arco 3 basta executar o seguinte comando:
    
    \begin{spverbatim}
# ./arc3.sh
    \end{spverbatim}

    
    O script se encarrega de incializar os bancos de dados nas configurações iniciais, chamar o \verb|health_checker.sh| (Script que verifica os status do banco e realiza a promoção automática) e o \verb|killdb.sh| (Script que mata o primário 2 vezes) para a realização dos testes. Os arquivos de saída do rbe são encontrados na pasta /ra137036/arc3/ da máquina cbn7. Vá para o diretório /ra137036/grupo06/resultados e execute o seguinte script:
    
    \begin{spverbatim}
# ./runAnalyse.sh
    \end{spverbatim}

    Para gerar os gráficos execute:
    \begin{spverbatim}
# gnuplot plot_graphs.plot
    \end{spverbatim}

     Os gráficos estarão disponíveis na pasta arc3/graphs.

\end{document}